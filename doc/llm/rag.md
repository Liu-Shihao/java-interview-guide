
# 相似性算法

## L2 欧氏距离 
欧几里得距离测量连接两点的线段的长度。
范围：[0, +∞]，值越小表示相似度越大

缺点：
受向量长度影响大，对长文档敏感；高维稀疏时效果差

## IP 内积
范围：[-1,1]，值越大表示相似度越大。

缺点：
未归一化，长文档得分天然偏高
## COSINE 余弦相似度
余弦相似度使用两组向量之间夹角的余弦来衡量它们的相似程度。你可以将两组向量想象成从同一点（例如 [0,0,…]）出发，但指向不同方向的线段。
范围：[0,1]，值越大表示相似度越大

余弦相似度始终在区间[-1, 1]内。例如，两个比例向量的余弦相似度为1，两个正交向量的相似度为0，两个相反向量的相似度为-1。余弦值越大，两个向量之间的夹角越小，表明这两个向量彼此越相似。

什么是高维向量？
高维向量是指维度（特征数量）显著较大的向量，通常在几十维到数千维甚至更高。
例如：
- 文本数据：BERT嵌入向量（768维或1024维）、TF-IDF向量（词汇表维度可能上万）。
- 图像数据：ResNet特征向量（2048维）、CLIP跨模态嵌入（512维）。

使用COS的好处：
1. 对向量文本长度不敏感，无论向量的长度是多长，余弦相似度的值始终在0到1之间。避免长文档支配相似度计算，更关注内容的相关性。
2. 余弦相似度可以处理高维向量，而欧氏距离只能处理低维向量。
3. 鲁棒性好，对停用词、标点等噪声不敏感。例子：“The cat sat.”和“Cat sat.”的余弦相似度接近1，而欧氏距离因长度差异可能较低。

什么是鲁棒性？
鲁棒性指系统或算法在输入数据存在噪声、异常或扰动时，仍能保持稳定性和有效性的能力。
即使文本存在拼写错误、停用词增减或表述差异，模型仍能返回正确的相似结果。

## JACCARD 
范围：[0,1]，值越大表示相似度越大

## BM25
范围：[0, +∞]，值越大表示相似度越大

- TF 词频

- IDF 逆文档频率

- 文档长度归一化


# Embedding
Embedding 模型选择有什么标准吗？

向量维度：
高维（768+）：表征能力强，但是计算成本高。
低维（128+）：适合轻量级应用，可能损失细粒度语义。

# RAG 评估指标
评估RAG（Retrieval-Augmented Generation）应用的质量需要综合考虑检索（Retrieval）和生成（Generation）两个模块的表现。

## 检索模块评估指标
1. Context Precision 上下文精度：检索结果中相关文档的比例。来识别检索的上下文是否相关。
2. Context Recall 上下文召回率：成功检索到相关文档的比例。更高的召回率表示遗漏的相关文档更少。

## 生成模块评估指标
1. 答案忠实性：生成答案是否严格基于检索内容，避免模型幻觉。
2. 答案相关性：生成答案是否直接回应问题，避免冗余或偏离。
3. 答案准确性：答案与标准答案的匹配程度
